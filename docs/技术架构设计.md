# 技术栈

## 核心框架与服务

* ​**开发语言**​: Python 3.10+
* ​**应用编排框架**​: **LangChain**
  * 用于构建 RAG 基础流程（加载、分割、向量化、检索、Prompt 组装）。
* **Web API**​​**​ 框架**​: **FastAPI**
  * 用于对外提供符合 RESTful 标准的后端 API，支持异步并发。
* ​**前端交互**​: **Streamlit**
  * 构建轻量级 WebUI，提供文件上传、参数配置和对话窗口的可视化操作。

## 模型与推理

* ​**模型推理框架**​: **Xinference**
  * 实现大模型的本地化部署，提供兼容 OpenAI API 的协议接口。
* ​**模型微调框架**​: **LlamaFactory** & **Swanlab**
  * LlamaFactory 用于 PEFT/LoRA 高效微调。
  * Swanlab 用于可视化训练过程跟踪与指标监控。
* **支持模型：**
  * ​**LLM**​: GLM-4-Chat, Qwen2.5-7B-Instruct (Int4)。
  * ​**Multimodal**​: Qwen2-VL-7B-Instruct。
  * ​**Embedding**​: bge-large-zh, Qwen3-text-embedding-8B。
  * ​**Rerank**​: BGE-Reranker​~~​ 或 Cross-Encoder~~​。

## 数据存储与检索

* ​**向量数据库**​: **Chroma**
  * 作为本地轻量级向量库，负责向量数据的持久化存储与索引维护
* ​**检索策略库**​:
  * ​**BM25 / Jieba**​: 用于关键词检索和分词。
  * ​**LangChain Retrievers**​: 实现混合检索和多策略检索。

# 项目目录结构设计

项目结构遵循模块化设计，将数据处理、模型服务、API 接口和前端展示分离，以便于维护和扩展。

```Plain
Project_Root/
├── app/
│   ├── api_server/                            # API 路由层 (FastAPI)
│   │   ├── __init__.py
│   │   ├── chat.py                     # 对话接口 /v1/chat/completions                
│   │   ├── upload.py                   # 文件上传与处理接口 /kb/upload
│   │   ├── search.py                   # 纯检索测试接口 /kb/search
│   │   └── dependencies.py             # 全局依赖注入（DB、向量库、模型实例等）
│   │
│   ├── configs/                        # 核心配置与基类
│   │   ├── config.yaml                 # 全局配置 (路径、阈值、模型名称等项目所有参数配置)
│   │   └── logging.yaml                # 日志配置（按模块输出级别）
│   │
│   ├── models/                         # 模型交互层 (LLM/Embedding/Rerank)
│   │   ├── llm/
│   │   │   ├── base.py                 # LLM 抽象基类 
│   │   │   └── openai_client.py        # OpenAI 协议兼容客户端（连接xinference实现模型部署调用）
│   │   ├── embedding/
│   │   │   └── local_embedding.py      # 本地 Embedding 模型调用 (bge-large-zh)
│   │   └── multimodal/
│   │       └── vlm_client.py           # 视觉大模型调用 (Qwen2-VL 图生文)
│   │
│   ├── rag/                            # RAG 核心引擎
│   │   ├── data_process/               # 数据处理 (Load -> Split)
│   │   │   ├── loaders/                # 文档加载器 (按文件类型拆分)
│   │   │   │   ├── base_loader.py      # 加载器基类
│   │   │   │   ├── pdf_loader.py       # PDF 解析 (PyMuPDF/PDFMiner) 
│   │   │   │   ├── markdown_loader.py  # Markdown 解析 
│   │   │   │   ├── word_loader.py      # Word 解析
│   │   │   │   ├── ppt_loader.py       # PPT 解析 
│   │   │   │   ├── csv_loader.py       # 结构化 CSV 过滤加载 
│   │   │   │   ├── image_loader.py     # OCR / 多模态图片处理
│   │   │   │   └── code_loader.py      # 代码文件加载与切分
│   │   │   ├── splitters/                # 文本切分策略 (按切分逻辑拆分)
│   │   │   │   ├── chinese_recursive.py  # 基础递归字符切分 (默认策略)  
│   │   │   │   ├── ali_text_splitter.py  # 基于阿里达摩院语义分割模型 
│   │   │   │   └── parent_child.py       # 父子块索引策略 (大小块切分) 
│   │   │   └── pipeline.py             # 自动化处理入口 (Load->Split->Vectorize) 
│   │   │
│   │   ├── storage/                    # 向量存储层
│   │   │   ├── chroma_store.py         # ChromaDB 增删改查封装
│   │   │   └── metadata_manager.py     # 元数据管理 (用于 ID 映射与过滤) 
│   │   │
│   │   ├── retrieval/                  # 检索层
│   │   │   ├── query_rewriter.py       # 查询改写模块
│   │   │   ├── search/                 # 具体的搜索算法实现
│   │   │   │   ├── vector_search.py    # 语义向量检索 (含 score_threshold) 
│   │   │   │   └── keyword_search.py   # 关键词检索 (Jieba + BM25) 
│   │   │   ├── multi_strategies/       # 检索策略组合
│   │   │   │   └── hybrid_strategy.py  # 混合检索逻辑 (向量 + 关键词加权) 
│   │   │   └── reranking/
│   │   │       └── bge_reranker.py     # 重排序模型调用 (BGE-Reranker 模型) 
│   │   ├── generation/                 # 生成层
│   │   │       ├── prompt_builder.py   # 动态 Prompt 组装 (系统词+文档+历史)
│   │   │       ├── file_chat.py        # 基于「上传临时文件」的 RAG 对话
│   │   │       └── QA_chain.py         # RAG 主流程（从用户输入到最终LLM回答生成全流程封装）
│   │   │
│   │   └── evaluation/                 # RAG效果评估模块
│   │           ├── generator.py        # 基于知识库自动生成测试集
│   │           └── eval_runner.py      # 评测执行器(计算 Faithfulness, Recall 等指标)
│   │
│   └── utils/                          # 通用工具          
│
├── webui/                              # 前端界面 (Streamlit)
│   ├── main.py                         # UI 启动入口
│   ├── pages/
│   │   ├── chat_interface.py           # 智能问答窗口逻辑
│   │   └── kb_manager.py               # 知识库管理台逻辑
│   │
│   └── components/                     # 复用 UI 组件
│       └── model_config.py             # 模型/参数配置界面 
│
├── finetune/                           # 模型微调相关
│   ├── dataset_prep.py                 # 微调数据格式化
│   ├── llama_factory_runner.py         # LlamaFactory 启动封装
│   └── monitor.py                      # Swanlab 监控集成
│
├── data/                               # 数据存放目录
│   ├── vector_db/                      # Chroma 数据库文件
│   ├── uploads/                        # 用户上传的临时文件
│   └── knowledge_db/                   # 本地的知识库文件
│
├── test/                               # 项目各个模块的测试
│   ├── test_xinference.py              # xinference调用大模型测试
│   └── qa_chat.py
│
├── docs/
│   ├── 产品功能说明.md
│   └── 技术架构设计文档.md
│
├── requirements.txt                    # 依赖列表
├── server.py                           # 项目启动入口 (启动 API 服务)
├── config.py                           # 实现对yaml配置文件的加载逻辑
└── README.md
```

## 核心模块实现逻辑

1. ### `app/api_server/` ：API接口路由层

该层负责接收 HTTP 请求，不包含复杂逻辑，主要负责参数校验和调用底层服务。核心提供三大类接口：对话、上传、检索。

* `chat.py`：对话接口
  * ​**核心功能**​：处理 `/v1/chat/completions` 对话请求
  * ​**实现逻辑**​：
    * 解析请求体，提取用户 `messages` 和 RAG 参数（如 `top_k`, `kb_id`）。
    * 调用 `rag.retrieval` 模块获取上下文片段。
    * 将上下文填入 Prompt，调用 `models.llm` 进行生成。
    * 支持流式（Streaming）响应，并附加“引用文档列表”作为额外字段返回。
    * 【新增】评估打分模式：
      * ​**新增入参**​：`eval_mode` (bool)，指示是否对本次对话进行实时打分。
      * 接收请求，执行正常的 RAG 检索与生成流程。
      * 若 `eval_mode=True`：
        * 在获取到完整 LLM 回答后，不立即关闭响应。
        * 将 `(question, answer, contexts)` 传递给 `app.rag.evaluation.eval_runner` 进行单条数据的快速评测。
        * 计算得出分数（如 `{faithfulness: 0.9, relevance: 0.8}`）。
      * ​**返回格式**​：将分数封装在响应的 `extra_info` 字段中返回给前端。注意：开启此模式会显著增加接口响应时间（等待评测模型推理）。
* `upload.py`：知识库上传接口
  * ​**核心功能**​：处理 `/kb/upload` 文件上传请求。
  * ​**实现逻辑**​：
    * 接收多格式文件（PDF/Word/MD等），保存至临时目录。
    * 根据文件后缀分发给 `rag.data_process.loaders` 中对应的加载器。
    * 触发`rag.data_process.pipeline`执行“加载 -> 切分 -> 向量化 -> 存储”的自动化流水线。
* `search.py`：检索测试接口
  * ​**核心功能**​：处理 `/kb/search` 纯检索请求。
  * ​**实现逻辑**​：
    * 仅执行检索逻辑，不调用 LLM 生成。
    * 用于调试检索效果，返回 Top-K 个文档片段及其相似度分数。

2. ### `app/configs/`

* `config.yaml`：全局配置文件
  * 该文件涵盖了文档中提到的模型管理、RAG 流程参数、知识库路径以及服务基础配置。
* `logging.yaml`：日志配置文件
  * 该文件实现了按模块分级输出日志，区分了控制台（开发用）和文件（生产排查用），并对第三方库（如 Chroma）进行了降噪处理。

在 `config.py` 中实现加载逻辑，使用 `PyYAML` 读取配置，并结合 `Pydantic` 进行类型校验。

3. ### `app/models/`

模型交互层，统一管理本地模型的调用。

#### `llm/`

* `base.py`：抽象基类
  * 它不包含具体的模型调用逻辑，而是定义了所有 LLM 客户端必须实现的方法。它强制规范了输入（Prompt/Messages）和输出（Response）的格式。
  * ​**统一入参**​：规定所有对话接口必须接受标准的 `messages` 列表（例如 `[{"role": "user", "content": "..."}]`）。
* `openai_client.py` ：封装了 **OpenAI API 协议** 的交互逻辑
  * ​**初始化**​：读取全局配置（`app/configs/config.yaml`），实例化官方的 `openai.OpenAI` 客户端。
  * ​**适配 chat 方法**​：调用 `client.chat.completions.create(stream=False)`，并解析返回的 JSON 结构，提取 `content`。
  * ​**适配 stream\_chat 方法**​：调用 `client.chat.completions.create(stream=True)`，通过 `yield` 逐个返回字符片段。
  * ​**错误处理**​：捕获 API 连接超时、鉴权失败等异常，并转换为系统内部的标准错误。
  * 可以通过修改配置文件 `config.yaml`，在“纯离线 Xinference 环境”和“在线 OpenAI 环境”之间切换。

#### `embedding/`

* `local_embedding.py`：本地 Embedding 模型封装
  * ​**实现逻辑**​：加载 `bge-large-zh` 或 `Qwen3-text-embedding-8B`（根据配置文件决定具体加载哪个），提供将文本转换为向量的统一接口。

#### `multimodal/vlm_client.py`：本地视觉大模型调用

​**实现逻辑**​：本地部署调用 Qwen2-VL 视觉模型，作为单独的一个模块，不参与RAG的检索增强，仅对输入的文本和图像进行推理生成。

4. ### `app/rag/`

#### **`data_process/ `**

* `loaders/` ：文档加载器，负责将不同格式的非结构化文件转换为统一的文本对象。
  * `base_loader.py`：加载器基类，统一管理以下不同的文档加载器。
  * `pdf_loader.py`：解析 PDF 文档。
    * **加载方式：PDF 文本 + 部分大图区域 OCR → 合并为文本 → ​**​**`unstructured`**​**​ 切分**
    * 继承 `UnstructuredFileLoader`
    * 使用 `fitz`（`PyMuPDF`）打开 PDF。
    * `ocr = get_ocr()` 获取 OCR 实例。
    * 返回整份 PDF 文本（正文 + 大图 OCR）拼在一起的 `resp`。
    * PDF 同时利用 PyMuPDF 提取「本身的文本层」+ 对较大的嵌入图片做 OCR，并解决页面旋转问题，然后把所有文字拼成一段文本，再交给 `unstructured` 进行文本级拆分。
  * `markdown_loader.py`：解析 Markdown 文件。
  * `word_loader.py`：解析word文档
    * 加载方式：`python-docx` 解析文字 + 表格 + 内嵌图片 OCR，再统一按纯文本切分
    * 继承自`langchain_community.document_loaders.unstructured.UnstructuredFileLoader`
    * 把整篇纯文本再拆分为 `Element` 列表（供 LangChain 后续转成 `Document`）。
  * `ppt_loader.py`：解析PPT文档
    * **​加载方式：​**​**`python-pptx`**​**​ 提取文本 + 表格文字 + 图片 OCR → 文本 → ​**​**`unstructured`**​**​ 切分**
    * 继承 `UnstructuredFileLoader`
    * 使用 `pptx.Presentation(filepath)` 打开 PPT/PPTX。
    * `RapidOCR()` 作为 OCR 引擎。
    * PPT 加载时，会遍历所有页面和形状，既收集文本框/表格里的文字，又对插入的图片做 OCR，然后统一作为纯文本再分段。
  * `csv_loader.py`：结构化 CSV 过滤加载
    * 基于标准库 `csv.DictReader` + 选定列拼接为文本
    * 继承自 `langchain_community.document_loaders.CSVLoader`
    * CSV 文件是按「行」拆成一条条 `Document`，内容是若干指定列的 `列名:值` 拼接，元数据里带行号、来源列等。
    * 每行生成一个 `langchain.docstore.document.Document`。
  * `image_loader.py`:
    * **加载方式：图片 → RapidOCR 识别 → 文本 → ​**​**`unstructured`**​**​ 切分**
    * 继承自`langchain_community.document_loaders.unstructured.UnstructuredFileLoader`
    * `get_ocr()` 获取一个 `RapidOCR` 实例。
  * `code_loader.py`：
* `splitters/`：文档切分器，在配置文件中选择不同的切分策略。
  * `chinese_recursive.py`：默认的基础切分策略。
    
    * ​**实现逻辑**​：
      * 继承 `RecursiveCharacterTextSplitter`
      * 分隔符优先级：`\n\n` → `\n` → `。|！|？` → `\.\s|\!\s|\?\s` → `；|;\s` → `，|,\s`
      * 支持正则分隔符
      * 保留分隔符（`keep_separator=True`）
      * 自动合并小于 chunk\_size 的片段
  * `ali_text_splitter.py`：基于阿里达摩院语义分割模型
    
    * 使用 `modelscope` 的 `nlp_bert_document-segmentation_chinese-base`
    * 按语义段落切分（非规则）
      注：`chinese_recursive.py`和`ali_text_splitter.py`在配置文件中选择一个使用。
  * `parent_child.py`：父子块索引策略
    
    * ​**实现逻辑**​：将文档切分为“小块”（Child，用于精准检索）和“大块”（Parent，用于上下文生成）。向量化小块，但建立 ID 映射，检索时返回对应的大块内容。
    * 这一策略是在现有切分结果基础上构建层次关系。
* `pipeline.py`：加载 -> （切分策略二选一）切分 -> 父子块索引（增强层） -> 向量化 -> 存储

#### `storage/`：向量存储层，负责数据的持久化与 CRUD 操作。

* `chroma_store.py`：封装 Chroma 数据库操作。
  * ​**数据库连接**​：负责初始化 Chroma Client，管理持久化路径。
  * ​**实现逻辑**​：
    * 实现文档的写入、删除。
    * 将 Chroma 实例转换为 LangChain 的 `Retriever` 接口。
    * 管理持久化路径和集合（Collection）。
  * **向量操作：**
    * ​**Add**​: 接收文本列表和 Embedding 向量，写入数据库。
    * ​**Query**​: 接收查询向量，返回最相似的 Top-K 个结果（包含 ID、Document、Metadata、Distance）。
    * ​**Delete**​: 根据 File ID 删除相关的向量数据（用于知识库文件的增删改查）。
* `metadata_manager.py`：负责 **RAG 系统中与“非向量信息”相关的逻辑处理**
  * **主要功能：**
    * **实现“父子块索引” (Parent-Child Indexing) 的映射逻辑**
      1. 当检索到“小块”向量时，`metadata_manager` 负责读取该小块 metadata 中的 `parent_id`。
      2. 利用这个 ID，从独立的键值存储（可以是内存字典、Redis 或 Chroma 的 Metadata 字段）中查找并提取完整的“大块”文本。
      3. 确保传递给 LLM 的是上下文更完整的 `Parent Chunk`，而不是破碎的 `Child Chunk`。
    * ​**结构化元数据的构建与清洗**​：文档要求支持 Markdown 标题切分，将 H1/H2/H3 作为元数据附加，确保检索时知道章节归属。同时需识别表格，原表格可能作为元数据存储
      1. 在写入数据前，标准化元数据字典。例如：`{"source": "产品手册.pdf", "page": 5, "h1": "部署", "h2": "环境要求", "type": "text"}`。
      2. 处理特殊内容：对于表格生成的摘要块，将其原始表格数据（Markdown/JSON 格式）存入 metadata，以便前端展示时能还原表格结构。
    * ​**构建过滤条件**​：在检索时支持配置过滤低相关性噪声
      1. 将前端传入的过滤参数（如“只搜 PDF 文件”或“只搜特定知识库”）转换为 Chroma 查询所需的特定 Filter 语法（如 `where={"source": "file.pdf"}`）。
    * 其他的功能逻辑待补充。
* 交互流程示例（以“父子块检索”为例）：
  * **准备阶段：**
    * `metadata_manager` 接收原始文档，生成 ​**小块**​（含 `parent_id`）和 ​**大块**​。
    * `metadata_manager` 指挥 `chroma_store` 将 **小块** 向量化存入向量库。
    * `metadata_manager` 将 **大块** 及其 ID 存入单独的存储区（DocStore 或通过 `chroma_store` 存入 metadata 字段）。
  * ​**检索阶段**​：
    * ​**Step 1**​: 业务层调用 `chroma_store.query()`，基于向量相似度找到了 5 个 ​**小块**​。
    * ​**Step 2**​: 业务层将这 5 个小块的结果交给 `metadata_manager`。
    * ​**Step 3**​: `metadata_manager` 查看结果中的元数据，提取出对应的 `parent_id`。
    * ​**Step 4**​: `metadata_manager` 获取对应的 **大块** 完整文本。
    * ​**Step 5**​: 最终返回给 Prompt 组装模块的是 **大块** 的内容。

#### `retrieval/`

* `query_rewriter.py` ：查询改写模块
  * ​**核心功能**​：在检索前优化用户输入，解决口语化、指代不清或语义缺失的问题。
  * ​**实现逻辑**​：
    * ​**Prompt 构建**​：使用专门的 System Prompt（如“你是一个搜索优化助手，请将用户的输入改写为更适合知识库检索的陈述句...”）。
    * ​**LLM 调用**​：复用 `app.models.llm` 接口请求大模型，生成改写后的 Query。
    * ​**熔断/降级机制**​：设置严格的超时时间（如 2秒）。如果 LLM 调用失败、超时或返回格式错误，​**强制捕获异常并返回原始 User Query**​，确保检索流程不中断。
* `search/vector_search.py`：语义检索实现
  * ​**实现逻辑**​：执行向量相似度计算，支持动态传入 `score_threshold`（过滤低相关性内容）和 `k` 值。
* `search/keyword_search.py`：关键词检索实现
  * ​**实现逻辑**​：使用 Jieba 进行中文分词，基于 BM25 算法对文本进行评分检索，弥补向量检索在精确匹配上的不足。
* `multi_strategies/hybrid_strategy.py`：混合检索逻辑
  * ​**实现逻辑**​：并行执行向量检索和关键词检索，对结果进行加权融合（RRF 或加权求和），输出综合排名靠前的文档
* `reranking/bge_reranker.py`：重排序
  * ​**实现逻辑**​：调用 BGE-Reranker 模型，对初步召回的候选文档进行二次精细打分排序，提升相关性。

#### **`generation/`**

* `prompt_builder.py`
  * ​**Prompt 构造​**​：动态组装 “系统提示词 + 检索到的文档片段 + 历史对话记录 + 用户问题”，生成最终发送给 LLM 的字符串。
  * 严格封装 System Prompt，将用户输入仅作为 `{question}` 变量填入，防止用户通过提示词覆盖系统指令。
* `file_chat.py`：基于「上传临时文件」的 RAG 对话
  * ​**功能**​：支持用户 ​**上传文件 -> 建立临时向量库 -> 针对这些文件进行问答**​。
  * 相当于将用户在当前对话窗口中上传的文档作为知识库，基于临时文件构建临时向量库，只在临时向量库中检索并进行完整的RAG流程。
* `QA_chain.py`：本地持久化知识库问答核心实现（RAG核心）
  * ​**主要功能**​：将离散的组件（检索器、重排序器、Prompt 构建器、LLM）串联起来，形成完整的业务闭环。
  * `QAChain` 类需要实现以下四个关键步骤的串联：
    * ​**查询优化(Query Rewrite)**​：调用 `query_rewriter` 尝试优化问题语义。若成功则使用新 Query 进行检索，失败则使用原 Query。
    * ​**检索召回 (Retrieve)**​: 调用 `HybridStrategy` 获取候选片段。
    * ​**精排与过滤 (Rerank & Filter)**​: 调用 `BGEReranker` 并执行 `score_threshold` 过滤。
    * ​**上下文映射 (Context Mapping)**​: ​**关键点**​，利用 `MetadataManager` 将检索到的“小块”映射回“大块” (Parent-Child Indexing) 。
    * ​**生成回答 (Generate)**​: 组装 Prompt 并调用 LLM（支持流式/非流式）。

#### `evaluation/` ：Ragas 评估模块

* ​**核心目标**​：实现 RAG 系统效果的自动化量化评估，指导模型微调和知识库优化。
* `generator.py` ：测试集自动生成
  * ​**实现逻辑**​：利用 Ragas 的 `TestsetGenerator`，直接读取 `app/rag/data_process/loaders` 加载的文档对象。
  * ​**本地化适配**​：配置 LangChain 的 `ChatOpenAI` 包装器指向本地 Xinference 接口，使用本地 LLM 生成包含“问题 (question)”、“标准答案 (ground\_truth)”和“上下文来源 (contexts)”的合成数据集，解决冷启动阶段无测试数据的问题。
* `eval_runner.py`：多维指标评测
  * 读取测试集中的 `question`。
  * 批量调用 `QA_chain.py` 获取系统的实际 `answer` 和检索到的 `contexts`。
  * 调用 Ragas 评估接口，计算四大核心指标：
    * ​**Context Recall (召回率)**​：检索到的内容是否包含了回答问题所需的信息。
    * **Context ​**​**Precision**​​**​ (精确率)**​：检索到的内容中有效信息的占比（评估噪点）。
    * ​**Faithfulness (忠实度)**​：生成的回答是否完全基于检索到的上下文（检测幻觉）。
    * ​**Answer Relevancy (相关性)**​：回答是否直接解决了用户的问题。
  * **​evaluate\_single\_turn(​**​​**query**​​**, answer, contexts)**​：
    * 除了批量跑测试集外，新增支持针对**单次对话**的实时评测函数。

5. ### `test/`

* `test_xinference.py`：不调用RAG、不添加历史记录，仅在本地终端测试xinference在本地部署的大模型是否能正常用。

6. ### `webui/` ： 前端交互层

基于 Streamlit 构建的用户界面。

* `main.py`：UI服务启动
* `pages/chat_interface.py`：智能问答窗口
  * ​**实现逻辑**​：
    * 展示用户与 AI 的对话气泡。
    * 渲染 AI 回答中的引用角标，点击可展开显示源文档片段。
* `pages/kb_manager.py`：知识库管理台
  * **实现逻辑：**
    * 提供文件拖拽上传区域。
    * 展示知识库列表，支持新建和删除操作。
* `components/model_config.py`：参数配置侧边栏
  * ​**实现逻辑**​：提供滑块和下拉框，实时调整 Chunk Size、Overlap、Top-K、检索模式（混合/语义）等参数。

7. ### `server.py`

启动 API 服务，加载路由，连接数据库，​**承载核心业务逻辑**​。

接收请求 -> **调用 QAChain** -> 返回 JSON

是整个后端逻辑的“大门”。

启动 Uvicorn

